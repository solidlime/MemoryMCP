#!/usr/bin/env python3
"""
ç®¡ç†è€…ç”¨ãƒ„ãƒ¼ãƒ« - ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¨ç®¡ç†ä½œæ¥­ç”¨ã®CLIã‚³ãƒãƒ³ãƒ‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ç®¡ç†ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
- ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡è¤‡è¡Œå‰Šé™¤ï¼‰
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å†æ§‹ç¯‰
- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ç§»è¡Œï¼ˆSQLite â‡” Qdrantï¼‰
- é‡è¤‡æ¤œçŸ¥
- ãƒ¡ãƒ¢ãƒªãƒãƒ¼ã‚¸
- çŸ¥è­˜ã‚°ãƒ©ãƒ•ç”Ÿæˆ

ä½¿ç”¨ä¾‹ï¼š
    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    python admin_tools.py clean --persona nilou --key memory_20251101172801

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å†æ§‹ç¯‰
    python admin_tools.py rebuild --persona nilou

    # SQLite â†’ Qdrant ç§»è¡Œ
    python admin_tools.py migrate --source sqlite --target qdrant --persona nilou

    # é‡è¤‡æ¤œçŸ¥
    python admin_tools.py detect-duplicates --persona nilou --threshold 0.85

    # ãƒ¡ãƒ¢ãƒªãƒãƒ¼ã‚¸
    python admin_tools.py merge --persona nilou --keys memory_001 memory_002 memory_003

    # çŸ¥è­˜ã‚°ãƒ©ãƒ•ç”Ÿæˆ
    python admin_tools.py generate-graph --persona nilou --format html
"""

import sys
import argparse
from typing import List, Optional
import sqlite3
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.utils import config_utils, persona_utils, vector_utils, analysis_utils
from src.utils.persona_utils import current_persona


def clean_memory(persona: str, key: str) -> None:
    """Remove duplicate lines in memory"""
    print(f"ğŸ§¹ Cleaning memory for persona: {persona}, key: {key}")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    db_path = persona_utils.get_db_path(persona)
    
    if not Path(db_path).exists():
        print(f"âŒ Database not found: {db_path}")
        return
    
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        
        # ãƒ¡ãƒ¢ãƒªã®å†…å®¹ã‚’å–å¾—
        cursor.execute('SELECT content FROM memories WHERE key = ?', (key,))
        row = cursor.fetchone()
        
        if not row:
            print(f"âŒ Memory not found: {key}")
            return
        
        content = row[0]
        lines = content.split('\n')
        original_count = len(lines)
        
        # é‡è¤‡å‰Šé™¤
        seen = set()
        unique_lines = []
        for line in lines:
            if line not in seen:
                seen.add(line)
                unique_lines.append(line)
        
        cleaned_content = '\n'.join(unique_lines)
        removed_count = original_count - len(unique_lines)
        
        # æ›´æ–°
        cursor.execute('UPDATE memories SET content = ? WHERE key = ?', (cleaned_content, key))
        conn.commit()
    
    print(f"âœ… Cleaned {removed_count} duplicate lines from {key}")
    print(f"   Original: {original_count} lines â†’ Cleaned: {len(unique_lines)} lines")


def rebuild_vector_store(persona: str) -> None:
    """Rebuild vector store"""
    print(f"ğŸ”¨ Rebuilding vector store for persona: {persona}")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    
    try:
        result = vector_utils.rebuild_vector_store()
        print(f"âœ… {result}")
    except Exception as e:
        print(f"âŒ Error rebuilding vector store: {e}")
        import traceback
        traceback.print_exc()


def migrate_backend(source: str, target: str, persona: str, upsert: bool = True) -> None:
    """Migrate data between backends"""
    print(f"ğŸ”„ Migrating {source} â†’ {target} for persona: {persona}")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    
    try:
        if source == "sqlite" and target == "qdrant":
            result = vector_utils.migrate_sqlite_to_qdrant()
            print(f"âœ… Migrated {result} memories to Qdrant")
        elif source == "qdrant" and target == "sqlite":
            result = vector_utils.migrate_qdrant_to_sqlite(upsert=upsert)
            print(f"âœ… Migrated {result} memories to SQLite")
        else:
            print(f"âŒ Invalid migration path: {source} â†’ {target}")
    except Exception as e:
        print(f"âŒ Migration error: {e}")
        import traceback
        traceback.print_exc()


def detect_duplicates(persona: str, threshold: float = 0.85, max_pairs: int = 50) -> None:
    """Detect duplicate or similar memories"""
    print(f"ğŸ” Detecting duplicates for persona: {persona} (threshold: {threshold})")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    
    try:
        result = analysis_utils.detect_duplicate_memories(threshold=threshold, max_pairs=max_pairs)
        print(result)
    except Exception as e:
        print(f"âŒ Error detecting duplicates: {e}")
        import traceback
        traceback.print_exc()


def merge_memories(persona: str, memory_keys: List[str], merged_content: Optional[str] = None,
                  keep_all_tags: bool = True, delete_originals: bool = True) -> None:
    """Merge multiple memories into one"""
    print(f"ğŸ”— Merging memories for persona: {persona}")
    print(f"   Keys: {', '.join(memory_keys)}")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    
    try:
        result = analysis_utils.merge_memories(
            memory_keys=memory_keys,
            merged_content=merged_content,
            keep_all_tags=keep_all_tags,
            delete_originals=delete_originals
        )
        print(f"âœ… {result}")
    except Exception as e:
        print(f"âŒ Merge error: {e}")
        import traceback
        traceback.print_exc()


def generate_knowledge_graph(persona: str, output_format: str = "html",
                             min_count: int = 2, min_cooccurrence: int = 1,
                             remove_isolated: bool = True) -> None:
    """Generate knowledge graph"""
    print(f"ğŸ“Š Generating knowledge graph for persona: {persona}")
    
    # personaã‚’è¨­å®š
    persona_utils.current_persona.set(persona)
    
    try:
        # ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        graph = analysis_utils.build_knowledge_graph(
            min_count=min_count,
            min_cooccurrence=min_cooccurrence,
            remove_isolated=remove_isolated,
            persona=persona
        )
        
        # å‡ºåŠ›å½¢å¼ã«å¿œã˜ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        if output_format == "html":
            import os
            from datetime import datetime
            from src.utils.persona_utils import get_db_path
            
            # Temporarily override persona context
            original_persona = current_persona.get()
            current_persona.set(persona)
            
            try:
                # Get persona memory directory
                db_path = get_db_path()
                persona_dir = os.path.dirname(db_path)
                
                # HTML file path (single file per persona)
                output_path = os.path.join(persona_dir, f"knowledge_graph.html")
                
                # Remove old graph file if exists
                if os.path.exists(output_path):
                    os.remove(output_path)
                
                result = analysis_utils.export_graph_html(
                    graph, 
                    output_path=output_path,
                    title=f"Knowledge Graph - {persona}"
                )
                print(f"âœ… Knowledge graph saved to: {result}")
            finally:
                current_persona.set(original_persona)
        else:  # json
            result = analysis_utils.export_graph_json(graph)
            print(f"âœ… Knowledge graph generated")
            print(result)
    except Exception as e:
        print(f"âŒ Error generating knowledge graph: {e}")
        import traceback
        traceback.print_exc()


def migrate_schema(persona: Optional[str] = None) -> None:
    """Migrate SQLite schema (add missing columns)"""
    from scripts.migrate_schema import migrate_database
    import os
    
    if persona:
        print(f"ğŸ”§ Migrating schema for persona: {persona}")
        current_persona.set(persona)
        db_path = persona_utils.get_db_path(persona)
        
        if not os.path.exists(db_path):
            print(f"âŒ Database not found: {db_path}")
            return
        
        migrate_database(db_path)
    else:
        # Migrate all personas
        from scripts.migrate_schema import migrate_all_personas
        print("ğŸ”§ Migrating schema for all personas...")
        migrate_all_personas()


def main():
    parser = argparse.ArgumentParser(
        description="Memory MCP ç®¡ç†è€…ç”¨ãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    subparsers = parser.add_subparsers(dest='command', help='åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰')
    
    # clean ã‚³ãƒãƒ³ãƒ‰
    clean_parser = subparsers.add_parser('clean', help='ãƒ¡ãƒ¢ãƒªã®é‡è¤‡è¡Œã‚’å‰Šé™¤')
    clean_parser.add_argument('--persona', required=True, help='Personaå')
    clean_parser.add_argument('--key', required=True, help='ãƒ¡ãƒ¢ãƒªã‚­ãƒ¼')
    
    # rebuild ã‚³ãƒãƒ³ãƒ‰
    rebuild_parser = subparsers.add_parser('rebuild', help='ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’å†æ§‹ç¯‰')
    rebuild_parser.add_argument('--persona', required=True, help='Personaå')
    
    # migrate ã‚³ãƒãƒ³ãƒ‰
    migrate_parser = subparsers.add_parser('migrate', help='ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œ')
    migrate_parser.add_argument('--source', required=True, choices=['sqlite', 'qdrant'], help='ç§»è¡Œå…ƒ')
    migrate_parser.add_argument('--target', required=True, choices=['sqlite', 'qdrant'], help='ç§»è¡Œå…ˆ')
    migrate_parser.add_argument('--persona', required=True, help='Personaå')
    migrate_parser.add_argument('--no-upsert', action='store_true', help='ä¸Šæ›¸ãã—ãªã„ï¼ˆSQLiteâ†’Qdrantã®ã¿ï¼‰')
    
    # detect-duplicates ã‚³ãƒãƒ³ãƒ‰
    detect_parser = subparsers.add_parser('detect-duplicates', help='é‡è¤‡ãƒ¡ãƒ¢ãƒªã‚’æ¤œå‡º')
    detect_parser.add_argument('--persona', required=True, help='Personaå')
    detect_parser.add_argument('--threshold', type=float, default=0.85, help='é¡ä¼¼åº¦é–¾å€¤ (0.0-1.0)')
    detect_parser.add_argument('--max-pairs', type=int, default=50, help='æœ€å¤§æ¤œå‡ºãƒšã‚¢æ•°')
    
    # merge ã‚³ãƒãƒ³ãƒ‰
    merge_parser = subparsers.add_parser('merge', help='è¤‡æ•°ã®ãƒ¡ãƒ¢ãƒªã‚’ãƒãƒ¼ã‚¸')
    merge_parser.add_argument('--persona', required=True, help='Personaå')
    merge_parser.add_argument('--keys', nargs='+', required=True, help='ãƒãƒ¼ã‚¸ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆ')
    merge_parser.add_argument('--content', help='ãƒãƒ¼ã‚¸å¾Œã®å†…å®¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•çµåˆï¼‰')
    merge_parser.add_argument('--no-keep-tags', action='store_true', help='ã‚¿ã‚°ã‚’çµåˆã—ãªã„')
    merge_parser.add_argument('--no-delete', action='store_true', help='å…ƒã®ãƒ¡ãƒ¢ãƒªã‚’å‰Šé™¤ã—ãªã„')
    
    # generate-graph ã‚³ãƒãƒ³ãƒ‰
    graph_parser = subparsers.add_parser('generate-graph', help='çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ')
    graph_parser.add_argument('--persona', required=True, help='Personaå')
    graph_parser.add_argument('--format', default='html', choices=['html', 'json'], help='å‡ºåŠ›å½¢å¼')
    graph_parser.add_argument('--min-count', type=int, default=2, help='æœ€å°ãƒªãƒ³ã‚¯å‡ºç¾å›æ•°')
    graph_parser.add_argument('--min-cooccurrence', type=int, default=1, help='æœ€å°å…±èµ·å›æ•°')
    graph_parser.add_argument('--keep-isolated', action='store_true', help='å­¤ç«‹ãƒãƒ¼ãƒ‰ã‚’ä¿æŒ')
    
    # summarize ã‚³ãƒãƒ³ãƒ‰ (Phase 28.4)
    summarize_parser = subparsers.add_parser('summarize', help='æœŸé–“åˆ¥ãƒ¡ãƒ¢ãƒªã‚’è¦ç´„')
    summarize_parser.add_argument('--persona', required=True, help='Personaå')
    summarize_parser.add_argument('--period', required=True, choices=['day', 'week'], help='è¦ç´„æœŸé–“')
    
    # migrate-schema ã‚³ãƒãƒ³ãƒ‰
    schema_parser = subparsers.add_parser('migrate-schema', help='SQLiteã‚¹ã‚­ãƒ¼ãƒã‚’ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
    schema_parser.add_argument('--persona', help='Personaåï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯å…¨Personaï¼‰')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == 'clean':
        clean_memory(args.persona, args.key)
    
    elif args.command == 'rebuild':
        rebuild_vector_store(args.persona)
    
    elif args.command == 'migrate':
        migrate_backend(args.source, args.target, args.persona, upsert=not args.no_upsert)
    
    elif args.command == 'detect-duplicates':
        detect_duplicates(args.persona, args.threshold, args.max_pairs)
    
    elif args.command == 'merge':
        merge_memories(
            args.persona,
            args.keys,
            merged_content=args.content,
            keep_all_tags=not args.no_keep_tags,
            delete_originals=not args.no_delete
        )
    
    elif args.command == 'generate-graph':
        generate_knowledge_graph(
            args.persona,
            args.format,
            args.min_count,
            args.min_cooccurrence,
            remove_isolated=not args.keep_isolated
        )
    
    elif args.command == 'summarize':
        # Phase 28.4: æœŸé–“è¦ç´„ã‚³ãƒãƒ³ãƒ‰
        from tools.summarization_tools import summarize_last_day, summarize_last_week
        
        if args.period == 'day':
            print(f"ğŸ“ Summarizing last day for persona: {args.persona}")
            summary_key = summarize_last_day(persona=args.persona)
        else:  # week
            print(f"ğŸ“ Summarizing last week for persona: {args.persona}")
            summary_key = summarize_last_week(persona=args.persona)
        
        if summary_key:
            print(f"âœ… Summary created: {summary_key}")
        else:
            print(f"âš ï¸  Failed to create summary")
    
    elif args.command == 'migrate-schema':
        migrate_schema(args.persona)


if __name__ == '__main__':
    main()
